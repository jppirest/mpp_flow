{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdd17df-a6aa-4fd2-8f3e-c3cde319dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_tick_bars(timestamps_i, ax, color):\n",
    "    for t in timestamps_i:\n",
    "        ax.axvline(x=t, color = color)\n",
    "    # ax.set_title(label, fontsize=20)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fa258d-c31e-4cff-a543-fae428fe4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1\n",
      " 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
      " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1\n",
      " 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
      " 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_points = 250 # Number of time points\n",
    "time = np.arange(n_points)\n",
    "\n",
    "# Generate base binary time series using Bernoulli distribution\n",
    "X1 = np.random.binomial(1, 0.6, n_points)\n",
    "X2 = np.zeros(n_points)\n",
    "X3 = np.zeros(n_points)\n",
    "\n",
    "# Introduce causal relationships\n",
    "for t in range(1, n_points):\n",
    "    # X2 is more likely to happen if X1 happened in the previous step\n",
    "    X2[t] = np.random.binomial(1, 0.7 * X1[t-1] + 0.1)\n",
    "    # X3 is more likely to happen if X2 happened in the previous step\n",
    "    X3[t] = np.random.binomial(1, 0.8 * X2[t-1] + 0.2)\n",
    "\n",
    "\n",
    "print(X1)\n",
    "\n",
    "X1 = [time[i] for i in range(len(X1)) if X1[i] > 0]\n",
    "X2 = [time[i] + 0.1 for i in range(len(X2)) if X2[i] > 0]\n",
    "X3 = [time[i] + 0.3 for i in range(len(X3)) if X3[i] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c66ed5-4c64-415d-b060-5bdf06d5e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows = 3, sharex = True)\n",
    "\n",
    "_plot_tick_bars(X1, ax[0], color = 'red')\n",
    "_plot_tick_bars(X2, ax[1], color = 'green')\n",
    "_plot_tick_bars(X3, ax[2], color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a6a48f-ac51-4f35-b834-d83ca5145426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    device = \"cpu\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7749ccfc-6d6b-4376-9003-14aa3a73b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class ProbRNN(nn.Module):\n",
    "    def __init__(self, memory_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.linear = nn.Sequential(nn.Linear(self.memory_size, 64), nn.Tanh())\n",
    "        self.lstm = nn.LSTM(input_size = 64, hidden_size = 128, num_layers = 2, batch_first = True)\n",
    "        self.linear_mu = nn.Sequential(nn.Linear(128, self.memory_size))\n",
    "        self.linear_std = nn.Sequential(nn.Linear(128, self.memory_size))\n",
    "        self.gmm_weights = nn.Parameter(nn.Softmax(dim=1)((torch.rand(size = (1, self.memory_size)))))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      x = self.linear(x)\n",
    "      x, _ = self.lstm(x)\n",
    "      mu = self.linear_mu(x)\n",
    "      std = self.linear_std(x)\n",
    "      std = torch.abs(std)\n",
    "\n",
    "\n",
    "      new_X = self.sample(mu, std)\n",
    "      mix_dist = self.build_distrib(mu, std)\n",
    "      log_prob = mix_dist.log_prob(new_X)\n",
    "\n",
    "      return new_X, log_prob\n",
    "\n",
    "\n",
    "    def build_distrib(self, mus, stds):\n",
    "      \n",
    "        ### This should not work if batch_size != 1. Yet to decide how to deal with this behaviour.\n",
    "\n",
    "      component_dist = torch.distributions.Normal(mus, stds)\n",
    "      mix_weight = torch.distributions.Categorical(torch.abs(self.gmm_weights))\n",
    "      mix_dist = torch.distributions.MixtureSameFamily(mix_weight, component_dist)\n",
    "      self.mix_dist = mix_dist\n",
    "\n",
    "      return mix_dist\n",
    "\n",
    "    def sample(self, mu, std):\n",
    "\n",
    "      X =  mu + torch.randn_like(std)*std\n",
    "\n",
    "      return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f855226f-8779-40d8-b405-186b172cf57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inter_times(t : torch.Tensor, t_end : int):\n",
    "    # t.sort()\n",
    "\n",
    "    t = t.cpu().numpy()\n",
    "    tau = np.diff(t, prepend = 0, append=t_end)\n",
    "    \n",
    "    return torch.tensor(tau, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed764e6-a3a4-409a-988c-d9d8dae769b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ = torch.tensor(X1.copy(), dtype = torch.float32)\n",
    "X2_ = torch.tensor(X2.copy(), dtype = torch.float32)\n",
    "X3_ = torch.tensor(X3.copy(), dtype = torch.float32)\n",
    "processes = [X1_, X2_, X3_]\n",
    "\n",
    "seq_lengths = [len(t) for t in processes]\n",
    "\n",
    "t_end_scalar = 30\n",
    "t_end = [t_end_scalar]*len(processes)\n",
    "\n",
    "taus = list(map(lambda p: get_inter_times(*p), zip(processes, t_end)))\n",
    "taus = [taus[i] for i in range(3)]\n",
    "\n",
    "padded_taus = pad_sequence(taus, batch_first=True)  # (B, L)\n",
    "# padded_taus[-1].cumsum(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b598d71-6331-4512-8a1b-24f433b46be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrangerMPP(nn.Module):\n",
    "\n",
    "    def __init__(self, processes, memory_dim : int = 10):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.processes = processes\n",
    "        self.memory_dim = memory_dim\n",
    "        self.n_processes = len(self.processes)\n",
    "        self.GrangerMatrix = nn.Parameter(torch.Tensor(self.n_processes, self.n_processes)) # nn.Parameter(nn.Parameter(torch.Tensor(self.n_processes, self.n_processes)))\n",
    "        self.models = nn.ModuleList([ProbRNN(self.memory_dim) for i in range(self.n_processes)])\n",
    "        self.sweep_dict = self.make_sweep_dict()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4) \n",
    "\n",
    "    def e_step(self, in_weights: torch.Tensor, points_current_pp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the causes for each event of the current_p\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        n = len(points_current_pp)\n",
    "        rv = []\n",
    "    \n",
    "        for i in range(n):\n",
    "            cause = F.gumbel_softmax(\n",
    "                in_weights,\n",
    "                hard = True\n",
    "            )\n",
    "            cause = torch.argmax(cause, dim = 0)\n",
    "            rv.append(cause)\n",
    "    \n",
    "        \n",
    "        \n",
    "        return rv\n",
    "\n",
    "    def em_step(self, n_steps):\n",
    "        dic = {}\n",
    "        \n",
    "        for i in range(self.n_processes):\n",
    "            dic[i] = []\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            self.causes = []\n",
    "            for i_proc in range(self.n_processes):\n",
    "                rv = self.e_step(\n",
    "                    self.GrangerMatrix[i_proc],\n",
    "                    self.processes[i_proc]\n",
    "                )\n",
    "                self.causes.append(rv)\n",
    "            \n",
    "            \n",
    "            for i_proc in range(self.n_processes):\n",
    "                \n",
    "                causes_to_ith = self.causes[i_proc] ## causes of ith_proc\n",
    "                \n",
    "                for j, cause_to_ith in enumerate(causes_to_ith):\n",
    "                    \n",
    "                    cause_to_ith = cause_to_ith.item()\n",
    "                    effect_j_on_i = self.sweep_dict[i_proc][cause_to_ith]\n",
    "                    \n",
    "                    if (cause_to_ith == i_proc) and j>=self.memory_dim:\n",
    "                        X_to_pass = processes[i_proc][j - self.memory_dim : j]\n",
    "                        X_to_pass = X_to_pass.flip(dims = (0,)) - X_to_pass[0]\n",
    "                        #self.X_to_pass = X_to_pass\n",
    "                        #print(X_to_pass.shape)\n",
    "                        loss = self.m_step(i_proc, X_to_pass.unsqueeze(0)) \n",
    "                        dic[i_proc].append(loss)\n",
    "                    \n",
    "                    elif len(effect_j_on_i) > j:\n",
    "                        X_to_pass = effect_j_on_i[j]\n",
    "                        #print(X_to_pass.shape)\n",
    "                        #self.X_to_pass = X_to_pass\n",
    "                        loss = self.m_step(i_proc, X_to_pass.unsqueeze(0))\n",
    "                        dic[i_proc].append(loss)\n",
    "                \n",
    "                if (step + 1) % 25 == 0 or step == 0:\n",
    "                                print(f'Step: {step + 1}, Model: {i_proc}, Loss: {loss}')\n",
    "                    \n",
    "    \n",
    "        return dic\n",
    "\n",
    "    def m_step(self, i_proc, X):\n",
    "        model = self.models[i_proc]\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        z, loss = model(X)\n",
    "        loss = -1*loss\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        if not (torch.isnan(loss) | torch.isinf(loss)):\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        else:\n",
    "            print(f'NaN found in epoch: {step}')    \n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def make_sweep_dict(self):\n",
    "        \n",
    "        dic = {}\n",
    "        for i in range(self.n_processes):\n",
    "            target = processes[i]\n",
    "            dic[i] = {}\n",
    "            for j in range(self.n_processes):\n",
    "                cause = self.processes[j]\n",
    "                dic[i][j] = self.sweep(target, cause)\n",
    "        return dic\n",
    "\n",
    "\n",
    "    def sweep(self, pa, pc):\n",
    "      events = []\n",
    "      for ia in pa:\n",
    "        events.append((ia, 'a'))\n",
    "      for ic in pc:\n",
    "        events.append((ic, 'c'))\n",
    "    \n",
    "      lim = self.memory_dim\n",
    "    \n",
    "      events.sort()\n",
    "      mem = []\n",
    "      ret = []\n",
    "      for [t, e] in events:\n",
    "    \n",
    "        if e == 'c':\n",
    "          if len(mem) >= lim:\n",
    "            mem.pop(0)\n",
    "          mem.append(t)\n",
    "    \n",
    "        if e == 'a':\n",
    "          ## memoria ainda nao esta cheia:\n",
    "          if len(mem) < lim:\n",
    "            continue\n",
    "          ## recupere os deltas desse tempo para o tempo causa\n",
    "          pp = []\n",
    "          for tc in mem:\n",
    "            pp.append(t - tc)\n",
    "          ret.append(pp)\n",
    "    \n",
    "      return torch.tensor(ret, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8352769-ccac-4f6f-b833-bcd4d8756f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mpp \u001b[38;5;241m=\u001b[39m GrangerMPP(processes)\n\u001b[0;32m----> 2\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mmpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mem_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36mGrangerMPP.em_step\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     71\u001b[0m         X_to_pass \u001b[38;5;241m=\u001b[39m effect_j_on_i[j]\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m#print(X_to_pass.shape)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m#self.X_to_pass = X_to_pass\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_to_pass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m         dic[i_proc]\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m, in \u001b[0;36mGrangerMPP.m_step\u001b[0;34m(self, i_proc, X)\u001b[0m\n\u001b[1;32m     84\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[i_proc]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 87\u001b[0m z, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39mloss\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/apps/conda/joao.pires/.envs/jpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mProbRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m   x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     19\u001b[0m   mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_mu(x)\n",
      "File \u001b[0;32m/apps/conda/joao.pires/.envs/jpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/apps/conda/joao.pires/.envs/jpt/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/apps/conda/joao.pires/.envs/jpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/apps/conda/joao.pires/.envs/jpt/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "mpp = GrangerMPP(processes)\n",
    "l = mpp.em_step(n_steps = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27daef-95e3-4359-ac3e-fde179c9ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(3):\n",
    "    plt.plot(l[i], color = colors[i])\n",
    "\n",
    "t = [len(l[i]) for i in range(3)]\n",
    "\n",
    "print(f'Len of losses: {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a37c47-259c-490d-8738-8590890d2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for proc in range(3):\n",
    "    proc_size = len(processes[proc])\n",
    "    loss_size = len(l[proc])\n",
    "    temp = [ sum(l[proc][i:i + proc_size])/proc_size for i in range(0, loss_size - proc_size, proc_size)]\n",
    "    final.append(temp)\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(3):\n",
    "    plt.plot(final[i], color = colors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e33ef6-ceb9-4439-ba04-d858d9f777ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 3, sharex = True)\n",
    "\n",
    "_plot_tick_bars(X1, ax[0], color = 'red')\n",
    "_plot_tick_bars(X2, ax[1], color = 'green')\n",
    "_plot_tick_bars(X3, ax[2], color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6977aa3-6962-46d8-b141-10322bf1d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_tick_bars_causes(timestamps_i, ax, cause):\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for idx, t in enumerate(timestamps_i):\n",
    "        ax.axvline(x=t, color = colors[cause[idx]])\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d12db-0a20-4278-b974-41d1dd70549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex = True)\n",
    "\n",
    "idx = 2\n",
    "proc_to_plot = processes[idx].detach().cpu().numpy()\n",
    "_plot_tick_bars(proc_to_plot, ax[0], color = colors[idx])\n",
    "_plot_tick_bars_causes(proc_to_plot, ax[1], cause = mpp.causes[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e8f07-6f2f-4204-a1df-ef53c4dddce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpp.GrangerMatrix.detach().cpu().numpy()\n",
    "plt.imshow(X)\n",
    "plt.axis('off')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8e983-3b32-4fff-897d-46c64cffc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpp.GrangerMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d22837-16b6-484b-a8c2-3eb70491f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpp.GrangerMatrix.detach().cpu().numpy()\n",
    "X = (X.T/X.sum(axis = 1)).T\n",
    "plt.imshow(X)\n",
    "plt.axis('off')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620d8a4-1bc5-433a-8489-0465de8464d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641acdd-e0a4-4bf9-9144-5c4bdac6af87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
