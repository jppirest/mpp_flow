{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0483ff-fdba-44e8-810b-7e42a3ccac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class GrangerMPP(torch.jit.ScriptModule):\n",
    "\n",
    "    def __init__(self, processes: List[torch.Tensor], memory_dim: int = 10):\n",
    "        super().__init__()\n",
    "        self.processes = processes\n",
    "        self.memory_dim = memory_dim\n",
    "        self.n_processes = len(self.processes)\n",
    "        self.GrangerMatrix = nn.Parameter(torch.Tensor(self.n_processes, self.n_processes))\n",
    "        self.models = nn.ModuleList([ProbRNN(self.memory_dim) for _ in range(self.n_processes)])\n",
    "        self.sweep_dict = self.make_sweep_dict()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        self.causes = torch.empty()  # This will now be handled outside of TorchScript methods\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def e_step(self, in_weights: torch.Tensor, points_current_pp: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return the causes for each event of the current process.\n",
    "\n",
    "        Args:\n",
    "            in_weights (torch.Tensor): The input weights tensor.\n",
    "            points_current_pp (torch.Tensor): The tensor representing current process points.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the causes for each event.\n",
    "        \"\"\"\n",
    "        n = points_current_pp.size(0)\n",
    "        rv = torch.empty(n, dtype=torch.long)\n",
    "\n",
    "        for i in range(n):\n",
    "            cause = F.gumbel_softmax(in_weights, hard=True)\n",
    "            cause = torch.argmax(cause, dim=0)\n",
    "            rv[i] = cause  # Directly assign to the preallocated tensor\n",
    "\n",
    "        return rv\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def compute_causes(self, n_processes: int, GrangerMatrix: torch.Tensor, processes: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        causes = []  # Use a local variable instead of self.causes\n",
    "        for i_proc in range(n_processes):\n",
    "            rv = self.e_step(GrangerMatrix[i_proc], processes[i_proc])\n",
    "            causes.append(rv)\n",
    "        return causes  # Return causes instead of setting self.causes\n",
    "\n",
    "    def em_step(self, n_steps: int):\n",
    "        dic = {i: [] for i in range(self.n_processes)}\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            # Use compute_causes and assign to self.causes outside the TorchScript method\n",
    "            self.causes = self.compute_causes(self.n_processes, self.GrangerMatrix, self.processes)\n",
    "\n",
    "            for i_proc in range(self.n_processes):\n",
    "                causes_to_ith = self.causes[i_proc]  # causes of ith_proc\n",
    "\n",
    "                for j, cause_to_ith in enumerate(causes_to_ith):\n",
    "                    cause_to_ith = cause_to_ith.item()\n",
    "                    effect_j_on_i = self.sweep_dict[i_proc][cause_to_ith]\n",
    "\n",
    "                    if (cause_to_ith == i_proc) and j >= self.memory_dim:\n",
    "                        X_to_pass = self.processes[i_proc][j - self.memory_dim : j]\n",
    "                        X_to_pass = X_to_pass.flip(dims=(0,)) - X_to_pass[0]\n",
    "                        loss = self.m_step(i_proc, X_to_pass.unsqueeze(0))\n",
    "                        dic[i_proc].append(loss)\n",
    "                    elif len(effect_j_on_i) > j:\n",
    "                        X_to_pass = effect_j_on_i[j]\n",
    "                        loss = self.m_step(i_proc, X_to_pass.unsqueeze(0))\n",
    "                        dic[i_proc].append(loss)\n",
    "\n",
    "                if (step + 1) % 25 == 0 or step == 0:\n",
    "                    print(f'Step: {step + 1}, Model: {i_proc}, Loss: {loss}')\n",
    "\n",
    "        return dic\n",
    "\n",
    "    def m_step(self, i_proc_: int, X: torch.Tensor) -> float:\n",
    "        model = self.models[i_proc_]\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        z, loss = model(X)\n",
    "        loss = -1 * loss\n",
    "        loss = loss.sum()\n",
    "\n",
    "        if not (torch.isnan(loss) | torch.isinf(loss)):\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        else:\n",
    "            print(f'NaN found in epoch')\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def make_sweep_dict(self):\n",
    "        dic = {}\n",
    "        for i in range(self.n_processes):\n",
    "            target = self.processes[i]\n",
    "            dic[i] = {}\n",
    "            for j in range(self.n_processes):\n",
    "                cause = self.processes[j]\n",
    "                dic[i][j] = self.sweep(target, cause)\n",
    "        return dic\n",
    "\n",
    "    def sweep(self, pa: torch.Tensor, pc: torch.Tensor) -> torch.Tensor:\n",
    "        events = []\n",
    "        for ia in pa:\n",
    "            events.append((ia, 'a'))\n",
    "        for ic in pc:\n",
    "            events.append((ic, 'c'))\n",
    "\n",
    "        lim = self.memory_dim\n",
    "\n",
    "        events.sort()\n",
    "        mem = []\n",
    "        ret = []\n",
    "        for t, e in events:\n",
    "            if e == 'c':\n",
    "                if len(mem) >= lim:\n",
    "                    mem.pop(0)\n",
    "                mem.append(t)\n",
    "\n",
    "            if e == 'a':\n",
    "                if len(mem) < lim:\n",
    "                    continue\n",
    "                pp = [t - tc for tc in mem]\n",
    "                ret.append(pp)\n",
    "\n",
    "        return torch.tensor(ret, dtype=torch.float)\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class ProbRNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, memory_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.memory_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.linear_mu = nn.Sequential(nn.Linear(128, self.memory_size))\n",
    "        self.linear_std = nn.Sequential(nn.Linear(128, self.memory_size))\n",
    "        # Using nn.Parameter to make sure these weights are correctly registered in TorchScript\n",
    "        self.gmm_weights = nn.Parameter(torch.softmax(torch.rand(1, self.memory_size), dim=1))\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.linear(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        mu = self.linear_mu(x)\n",
    "        std = torch.abs(self.linear_std(x))\n",
    "\n",
    "        new_X = self.sample(mu, std)\n",
    "        log_prob = self.compute_log_prob(new_X, mu, std)\n",
    "\n",
    "        return new_X, log_prob\n",
    "\n",
    "    \n",
    "    def compute_log_prob(self, x: torch.Tensor, mus: torch.Tensor, stds: torch.Tensor) -> torch.Tensor:\n",
    "        # Manually compute the log probability for each component\n",
    "        component_log_probs = -0.5 * (((x - mus) / stds) ** 2 + 2 * torch.log(stds) + torch.log(torch.tensor(2 * torch.pi)))\n",
    "        # LogSumExp trick for computing log probabilities of the mixture\n",
    "        log_weights = torch.log(self.gmm_weights)\n",
    "        log_prob = torch.logsumexp(component_log_probs + log_weights, dim=-1)\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
    "        return mu + torch.randn_like(std) * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a960e5-9148-4042-aec4-2d3c72070bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GrangerMPP(processes=[torch.rand(10) for _ in range(5)], memory_dim=2)\n",
    "scripted_model = torch.jit.script(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b9c311-1ec1-4720-9b7c-22acc7f3db64",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mem_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mGrangerMPP.em_step\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcauses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_causes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_processes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGrangerMatrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_processes):\n\u001b[0;32m---> 57\u001b[0m     causes_to_ith \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcauses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_proc\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# causes of ith_proc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, cause_to_ith \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(causes_to_ith):\n\u001b[1;32m     60\u001b[0m         cause_to_ith \u001b[38;5;241m=\u001b[39m cause_to_ith\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.em_step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca36325-8aab-4701-87d3-51ab299f92b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
